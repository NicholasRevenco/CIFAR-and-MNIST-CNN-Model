{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Densely Connected Networks\n",
        "This is a Python script written in Google Colab to build, train, and evaluate a simple neural network model for image classification using the MNIST dataset. The MNIST dataset consists of images of handwritten digits (0-9) that are commonly used for training various image processing systems. Let's break down the code and explain what each part does:\n",
        "\n"
      ],
      "metadata": {
        "id": "y1dlfCjTV45D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and add some helper functions to the runtime.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Helper functions\n",
        "def show_min_max(array, i):\n",
        "    random_image = array[i]\n",
        "    print(random_image.min(), random_image.max())\n",
        "\n",
        "def plot_image(array, i, labels):\n",
        "    plt.imshow(np.squeeze(array[i]))\n",
        "    plt.title(\" Digit \" + str(labels[i]))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "AiRJRZDhXUJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code starts by importing necessary libraries: TensorFlow, Keras, NumPy, and Matplotlib. Additionally, two helper functions are defined - show_min_max and plot_image - which will be used later to visualize data.\n",
        "\n"
      ],
      "metadata": {
        "id": "JG02HuN8XWjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rows and columns for each image\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "# Number of output classes\n",
        "num_classes = 10\n",
        "\n",
        "# Load data into the program\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "(train_images_backup, train_labels_backup), (test_images_backup, test_labels_backup) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "XDoMkcoxXWMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the code sets the dimensions for each image (img_rows and img_cols) to 28x28 pixels, and the number of output classes (num_classes) to 10 since this is a handwritten digit classification problem (0 to 9). The MNIST dataset is loaded, split into training and test sets, and stored in corresponding variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "qTUyFvU8XZZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of data\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "\n",
        "# Value of input shape to shape of grid\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Reshape the data from a long list of pixels to a 28x28 grid.\n",
        "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
        "# Add print statements to see the new shape.\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_420qvYXaWc",
        "outputId": "f3e66815-7104-42f6-bf6d-619e988c38ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shapes of the training and test data are printed to understand their dimensions. The input_shape variable is set to (28, 28, 1) to represent the dimensions of each image, including the single channel (grayscale).\n",
        "\n",
        "Next, the training and test images are reshaped to be 4-dimensional arrays, where the first dimension represents the number of images, the second and third dimensions represent the width and height of each image, and the last dimension represents the number of channels (1 for grayscale)."
      ],
      "metadata": {
        "id": "sbfcJeTIXbn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of image\n",
        "plot_image(train_images, 100, train_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "rA0y2aYTXcsQ",
        "outputId": "5aefa74a-40c3-4fab-fd76-8da9b1b5f169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOLElEQVR4nO3dbYyVZXrA8evMHAZBB8qbrRQyCEhkdTGCYtDaUNsgYddobDRQiU6TtjYN2ExTbdQ1IbURtakfaNE0SETR1HbVapqiBlcmdZXNoqS2bpc2us5oO9FWARlLcRjO0w9brnaKA/Mc5gWG3y+ZD5w51zw3LzP/uc8DN5WiKIoAgIhoGOkFAHDqEAUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgVGtfb29qhUKtHe3l56tqOjIyqVSmzZsmXQ1wWnKlHgtHH0i/TRtzFjxsTUqVPjyiuvjHvuuSc++uijIV/Dtm3bYt26dQN+fmtra581H3278MILh26RcBIqzj7idNHR0RHnn39+rFq1KlasWBG1Wi327dsXu3btihdeeCEqlUps3rw5Vq5cmTO1Wi16enqiqakpGhrKfQ9UFEV89dVXMWbMmGhsbIyIiDVr1sTGjRtjoJ82ra2t8eyzz8bjjz/e5/GJEyfGddddV2o9MByqI70AKGvhwoWxevXqPo91dnbGsmXL4rbbbov58+fHJZdcEhERDQ0NcdZZZ9V1nUqlUvfs/1WtVo9ZL5yqvHzEqNDS0hJbtmyJnp6eePjhh/Px/u4pbNy4MWbPnh3jxo2LxYsXxxtvvBFLly6NpUuX5nP+/z2F1tbW2LhxY0REn5eCBuLIkSNx4MCBk/o5wnCwU2DUWLJkScyZMye2b99+3Oc99thjsWbNmrj66qujra0tOjo64oYbbohJkybFjBkz+p27/fbbo6urK7Zv3x5bt24d8LoOHjwYEyZMiIMHD8akSZNi1apV8dBDD8U555wz4I8Bw0UUGFUuvvjieOmll+LAgQMxYcKEY97f09MT9913X1x++eXx+uuvR7X600+BBQsWRGtr63GjsGTJkpg3b15s3759wC8HnXfeeXHXXXfFwoULo1arxSuvvBKPPvpovPvuu9He3p7Xh1OFP5GMKke/++7u7v7aKLz99tvx+eefx/r16/t8Qb7llluira1t0Nezfv36Pj9euXJlzJs3L+6999547rnn+twUh1OBewqMKl9++WVERDQ3N3/t+zs7OyMiYu7cuX0er1arMWvWrCFd21FtbW3R0NAQr7322rBcD8oQBUaV9957L84999yv3SWcKsaNGxdTpkyJvXv3jvRS4BiiwKixc+fO+OCDD2LZsmX9PqelpSUiIt5///0+j/f29kZHR8cJrzHQv210PN3d3fHZZ5/FtGnTTvpjwWATBUaFzs7OaG1tjaamprjzzjv7fd5ll10WU6ZMiU2bNkVvb28+/swzz8S+fftOeJ2zzz47IiL2799/wuceOnQouru7j3n8/vvvj6IoYvny5Sf8GDDc3GjmtLN79+54+umno1arxf79+2PXrl3x/PPPR6VSia1bt8aCBQv6nW1qaop169bF2rVr45prrombb745Ojo6YsuWLTFnzpwT7gQWLVoUERF33HFHXHvttdHY2NjvzeJPPvkkLr300li1alUea/Hqq6/Gtm3bYvny5XH99dfX+SsAQ6iA08SHH35YRES+VavVYvLkycUVV1xR3H333UVnZ+cxMzt27CgiotixY0efxzds2FC0tLQUY8eOLRYvXly8+eabxaJFi4rly5cfc70nnngiH+vt7S3Wrl1bTJs2rahUKsXxPoX27dtXrF69upg7d24xfvz4YuzYscVFF11UPPDAA0VPT89J/3rAUHD2EcRPz0iaNm1a3HjjjbFp06aRXg6MGPcUOOMcOnTomAPtnnrqqdi7d2+fYy7gTGSnwBmnvb092tra4qabboopU6bE7t27Y/PmzTF//vx45513oqmpaaSXCCPGjWbOOLNmzYqZM2fGhg0bYu/evTF58uS49dZb48EHHxQEznh2CgAk9xQASAN6+ahWq0VXV1c0NzcPyr/oBGB4FUUR3d3dMX369OP+L4QDikJXV1fMnDlz0BYHwMj4+OOPj3tE/ICicPTEyV+IFVGNMYOzMgCGTW8cju/Htn5PED5qQFE4+pJRNcZEtSIKAKed//krRSe6BeBGMwBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUnWkFwBDoqGx9Ej1Z6eVnumZ83OlZ96/pan0TL3e+NYjpWdmVM8pPfPB4S9Lz1z/2F2lZyIifv7Bt+qaY2DsFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkByIx7BpnFb+wLmIiH/7tQtKzxS/tK/0zDuXP1165lT3L4fLHwz42oFzS8+8f+ibpWdmvlz+9ygiolbXFANlpwBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACSnpDJs9tw3u665f/7VPx3klYysHx8+XNfck59fWXrmne8sKj0z9uVdpWfq8+Nhug5l2CkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACA5EI+6fPjsgtIzP7jqkTqvdlbpiS9qh0rP/OKf31l6Zso/HSk9M+7Tr0rPRERU3vz70jNjY7gOt2O0sFMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEByIB51ufUbPyw9M6mh/MF29Xqvp7n0zMw/emsIVgKnFzsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkB+JRl6f3XF565g+u+tEQrOTr/cZf/1bpmTnxgyFYCZxe7BQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDklFTqMq69ufzQVfVd66vicOmZGd87Ut/F4AxnpwBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgORAPE55h4ryh9uNfXnXEKwERj87BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKk60gvg9DT9bz4qPbPz9xvrutYlTeW/d2lYcGHpmdo/7Ck9A6ONnQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJCckkpdej/+19Iz+4+Mr+ta4ytHSs/c/eKzpWfe/a+W0jP12PC3K+qau+BPPig9c+TTf6/rWpy57BQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJAqRVEUJ3rSgQMHYuLEibE0ro9qZcxwrItR6MtXZtc11/7N7w7ySk5Pv975y6VnPnp4XumZcS/+sPQMp77e4nC0x0vxxRdfxIQJE/p9np0CAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBSdaQXwJnjnBWddc1d/IdrSs9M/tEJz3k8xn8srJSe+c3lr5We+b3Je0rPREQ80fK90jPzvnVB+ZkXS48witgpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgORCP4VM7UtfYrO/sHOSFfL0Jf1F+5u8eu6j0zHnb9pW/UESsav609Ezr4jdLz+ysji89U/T2lp7h1GSnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5EA8OAm9P+koPfPQkzfXda3lv/PHpWfumfqPpWeua7yy9Ew4EG/UsFMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSU1JhmM1Y/1Zdc3+5+hulZ377Z35S17U4c9kpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgORAPhlnj3PPrmps9ds8grwSOZacAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDkQDwYZnt+99y65paN+8/SM4/svbD8hY4cKT/DqGGnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5EA8GGZT367ze7Eby4/81Z/9SumZqb07y1+IUcNOAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASE5JhWE26cn6TiH99pOLSs9MDSeeUo6dAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACBVB/KkoigiIqI3DkcUQ7oeAIZAbxyOiP/9et6fAUWhu7s7IiK+H9tOclkAjKTu7u6YOHFiv++vFCfKRkTUarXo6uqK5ubmqFQqg7pAAIZeURTR3d0d06dPj4aG/u8cDCgKAJwZ3GgGIIkCAEkUAEiiAEASBQCSKACQRAGA9N873hJ/SI/w+AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the plot_image helper function to display the 100th training image along with its corresponding label.\n",
        "\n"
      ],
      "metadata": {
        "id": "V48PTEIFXdd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How the computer would see the data\n",
        "out = \"\"\n",
        "for i in range(28):\n",
        "    for j in range(28):\n",
        "        f = int(train_images[100][i][j][0])\n",
        "        s = \"{:3d}\".format(f)\n",
        "        out += (str(s)+\" \")\n",
        "    print(out)\n",
        "    out = \"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ3cVTuKXeIt",
        "outputId": "227149e0-5598-4af6-adf5-040a6d460e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   2  18  46 136 136 244 255 241 103   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0  15  94 163 253 253 253 253 238 218 204  35   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0 131 253 253 253 253 237 200  57   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0 155 246 253 247 108  65  45   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0 207 253 253 230   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0 157 253 253 125   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0  89 253 250  57   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0  89 253 247   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0  89 253 247   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0  89 253 247   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0  21 231 249  34   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0 225 253 231 213 213 123  16   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0 172 253 253 253 253 253 190  63   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   2 116  72 124 209 253 253 141   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  25 219 253 206   3   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 104 246 253   5   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 213 253   5   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  26 226 253   5   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 132 253 209   3   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  78 253  86   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section prints out the pixel values of the 100th training image in a grid format, showing how the computer views the handwritten digit.\n",
        "\n"
      ],
      "metadata": {
        "id": "-UQwAiz3Xflj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display maximum and minimum values\n",
        "show_min_max(train_images, 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-e54Z04XgVa",
        "outputId": "280c1656-9f97-4897-aa7d-13591604f298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The show_min_max helper function is used to display the minimum and maximum pixel values of the 100th training image.\n",
        "\n"
      ],
      "metadata": {
        "id": "gAsUpG9AXhXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to float32\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "\n",
        "# Normalize the pixel values to the range [0, 1]\n",
        "train_images /= 255\n",
        "test_images /= 255\n",
        "\n",
        "# Convert the labels to one-hot encoded format\n",
        "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n"
      ],
      "metadata": {
        "id": "iizJxBK_XiU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pixel values of the training and test images are converted to float32 and then normalized to the range [0, 1] by dividing by 255 (maximum pixel value). Additionally, the labels are converted from integer format to one-hot encoded format, which is required for categorical classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "SRTLsr33XjS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tenserflow libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "\n",
        "# Training is done in a sequence of rounds called epochs. Each epoch is one pass over the dataset. Meaning, in each epoch, every image in the dataset is passed through your model once. Generally, the more epochs you run, the better your results, but the longer it will take to train. Finding the sweet spot between good results and reasonable runtime is a big challenge in training a model.\n",
        "epochs = 10\n",
        "\n",
        "# You'll be making a sequential model. Sequential models are split into layers. A layer is one set of neurons that processes the inputs from the previous layer, then passes it along to the next layer. The first layer reads in the original data, and the final layer produces the network's prediction.\n",
        "model = Sequential()\n",
        "\n",
        "# Add the input shape to the layer being added in the cell below.\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Dropout(rate=0.3))\n",
        "\n",
        "# Add another Conv2D layer\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-cii9svXkBX",
        "outputId": "6f20faf7-4ae9-4d9e-a45a-c41cc11dd919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_21 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 13, 13, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 11, 11, 64)        0         \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 9, 9, 32)          18464     \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 2592)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 32)                82976     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,586\n",
            "Trainable params: 120,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the code sets the number of training epochs to 10. It defines a sequential model with two dense layers. The first layer is a flatten layer that takes the input shape of (28, 28, 1). The second layer is a dense layer with 16 units and a ReLU activation function. The third layer is another dense layer with 10 units (equal to the number of output classes) and a softmax activation function, which is suitable for multiclass classification. The model.summary() displays a summary of the model architecture.\n",
        "\n"
      ],
      "metadata": {
        "id": "9G8rWpp6XlZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, batch_size=64, epochs=epochs, validation_data=(test_images, test_labels), shuffle=True)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "scores = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdtbCwP4XmFk",
        "outputId": "21bbcb13-8a75-46a5-deec-9a4e885eeafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 8s 5ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.0303 - val_accuracy: 0.9928\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0343 - val_accuracy: 0.9922\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0375 - val_accuracy: 0.9923\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0357 - val_accuracy: 0.9930\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0375 - val_accuracy: 0.9929\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0382 - val_accuracy: 0.9925\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0358 - val_accuracy: 0.9930\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0461 - val_accuracy: 0.9922\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0430 - val_accuracy: 0.9929\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0464 - val_accuracy: 0.9929\n",
            "313/313 - 1s - loss: 0.0464 - accuracy: 0.9929 - 688ms/epoch - 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is compiled with the Adam optimizer, categorical cross-entropy loss function, and accuracy metric. It is then trained on the training data using the model.fit() function, with the specified number of epochs. Finally, the model's performance is evaluated on the test data using the model.evaluate() function, and the test loss and accuracy are printed."
      ],
      "metadata": {
        "id": "GoGwCczcXnE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model.h5')\n"
      ],
      "metadata": {
        "id": "T3BNXA53XpXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trained model is saved to a file named 'my_model.h5' using the model.save() function.\n",
        "\n"
      ],
      "metadata": {
        "id": "yyC67dL7XqTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trained CNN model is saved to a file named \"cnn_model.h5\" using the save() method of the model. This saved model can be loaded and used for predictions on new images later. You can create your own test images, or you can use these example images: https://drive.google.com/drive/folders/1P1p161W5SSt2wFh7W3Z7-6gFVsv7kVsW.\n"
      ],
      "metadata": {
        "id": "nyrdBId2fIrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual graph representation of the model's output\n",
        "def plot_value_array(predictions_array, true_label, h):\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array[0], color=\"#777777\")\n",
        "  plt.ylim([(-1*h), h])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')\n",
        "  plt.show()\n",
        "  print(\"The predicted number is the number \", predicted_label)\n",
        "\n",
        "# Ability to upload images\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "model = keras.models.load_model('my_model.h5')\n",
        "for filename in uploaded.keys():\n",
        "    path = filename\n",
        "\n",
        "# Modify the image loading code to use the uploaded image\n",
        "img = tf.keras.preprocessing.image.load_img(path, target_size=(28, 28), color_mode=\"grayscale\")\n",
        "img_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "# Perform the prediction using the loaded image\n",
        "arr = model.predict(np.expand_dims(img_arr, axis=0))\n",
        "\n",
        "# Call of graph function\n",
        "plot_value_array(arr, 0, num_classes)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "twT1Uh4KfMW1",
        "outputId": "ae3e4044-425e-4ce5-bf8c-0775401b9515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f6e4fb2d-d24b-4e0c-9817-c1fb5ed3f88c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f6e4fb2d-d24b-4e0c-9817-c1fb5ed3f88c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test3.png to test3 (4).png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x79066c9e7a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 100ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted number is the number  3\n"
          ]
        }
      ]
    }
  ]
}