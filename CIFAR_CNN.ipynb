{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding CIFAR Data\n",
        "\n",
        "CIFAR is a dataset that consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. It is a more general dataset compared to MNIST and suitable for a more complex image recognition problem.\n",
        "\n",
        "We're going to follow a similar process as we did with the MNIST dataset to prepare this data for our neural network.\n"
      ],
      "metadata": {
        "id": "_h9E3N3uN14Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules from specific libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "mYZcrnJORXdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first part of the script is dedicated to importing the necessary modules from the libraries we're going to use. TensorFlow and Keras are two of the most commonly used libraries for building deep learning models. We import the CIFAR-10 dataset from Keras. The matplotlib library is used for plotting, and we'll use numpy for mathematical operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "GSxZ5b_yRYjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def show_min_max(array, i):\n",
        "  random_image = array[i]\n",
        "  print(\"min and max value in image: \", random_image.min(), random_image.max())\n",
        "\n",
        "def plot_image(array, i, labels):\n",
        "  plt.imshow(np.squeeze(array[i]))\n",
        "  plt.title(str(label_names[labels[i]]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "1dex2-RFRZfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These helper functions are defined to help visualize the data in the CIFAR-10 dataset. The show_min_max function prints the minimum and maximum values in an image, which can help understand the data range. The plot_image function is used to display an image from the dataset, along with its label.\n",
        "\n"
      ],
      "metadata": {
        "id": "fVeyac81RaVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 32\n",
        "img_cols = 32\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "(train_images_backup, train_labels_backup), (test_images_backup, test_labels_backup) = cifar10.load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzE4V51GRbA-",
        "outputId": "e1841d17-be96-496c-8434-d042cd84d6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We specify some constants regarding the images' dimensions and the number of classes in the CIFAR-10 dataset. The CIFAR-10 dataset, containing 60,000 32x32 color images in 10 classes, is loaded using cifar10.load_data().\n",
        "\n"
      ],
      "metadata": {
        "id": "cd-m3-IlRcEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the names of labels in CIFAR-10 dataset\n",
        "label_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Flatten the label lists\n",
        "train_labels_backup = [item for sublist in train_labels_backup for item in sublist]\n",
        "test_labels_backup = [item for sublist in test_labels_backup for item in sublist]\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "\n",
        "# Display the image at index 100 and the min and max pixel values of it\n",
        "plot_image(train_images, 100, train_labels_backup)\n",
        "show_min_max(train_images, 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "w_TG27bzRczs",
        "outputId": "8e66e2a3-f56a-4626-a22b-fe4b89417e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min and max value in image:  30 242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines the labels corresponding to the ten classes in the CIFAR-10 dataset. It flattens the label lists and prints the shape of the training and test images. The image at index 100 and the min and max pixel values of it are displayed.\n",
        "\n"
      ],
      "metadata": {
        "id": "uBM_f6qFRe06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the image matrices into float32 type\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "\n",
        "# Normalize the pixel values\n",
        "train_images /= 255\n",
        "test_images /= 255\n",
        "\n",
        "# Display the image at index 100 and the min and max pixel values of it after normalization\n",
        "plot_image(train_images, 100, train_labels_backup)\n",
        "show_min_max(train_images, 100)\n",
        "\n",
        "# Convert the labels into categorical type\n",
        "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "aQ9qfWlwRfyJ",
        "outputId": "73a969c3-b03b-476a-baf1-91d3837ac2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min and max value in image:  0.11764706 0.9490196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section converts the images to floating-point numbers and normalizes them so that pixel values are between 0 and 1, which is an important preprocessing step for neural network inputs. It then converts the labels into one-hot vectors, which are binary matrices representing the class of each image.\n",
        "\n"
      ],
      "metadata": {
        "id": "HyrlWspERjeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules to build the CNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n"
      ],
      "metadata": {
        "id": "3it6ighlRk3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we import the necessary modules to build our CNN model using Keras.\n",
        "\n"
      ],
      "metadata": {
        "id": "HDz_xUwuRl6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "batch_size = 64\n",
        "\n",
        "# Initiate the model\n",
        "model = Sequential()\n"
      ],
      "metadata": {
        "id": "LDgbrUpURmrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define some hyperparameters for the model training, such as the number of epochs (the number of times the model will go through the entire training dataset) and the batch size (the number of training examples utilized in one iteration). The Sequential model is a linear stack of layers that we can start adding to.\n",
        "\n"
      ],
      "metadata": {
        "id": "NRuC0lP9Rnxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(rate=0.2))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=num_classes,activation='softmax'))\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2SF_SBVRpDE",
        "outputId": "dafc0277-664e-4c2e-d83c-a6b7cb38cb58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 14, 14, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 10, 10, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 64)          73792     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8, 8, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               524416    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 730,442\n",
            "Trainable params: 730,058\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After building our CNN model, we use model.summary() to print a summary representation of the model. This will show the structure of the model, layer-by-layer, along with the number of parameters (weights and biases) in each layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "GjV1ZFRuRqbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "kp3Je2_GRzy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is compiled with the Adam optimizer, a commonly used optimizer because of its computational efficiency and good performance on a wide range of problems. The categorical cross-entropy is used as the loss function which is suitable for multi-class classification problems. The metric used to evaluate the model is accuracy, the fraction of images that are correctly classified.\n",
        "\n"
      ],
      "metadata": {
        "id": "G81xE4NDR0rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(test_images, test_labels), shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP6qTHnmR21N",
        "outputId": "63034956-3021-42c0-99c7-d7921fade773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "782/782 [==============================] - 24s 13ms/step - loss: 1.5333 - accuracy: 0.4563 - val_loss: 1.1958 - val_accuracy: 0.5720\n",
            "Epoch 2/15\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 1.0656 - accuracy: 0.6201 - val_loss: 1.2949 - val_accuracy: 0.5709\n",
            "Epoch 3/15\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.8855 - accuracy: 0.6853 - val_loss: 0.9707 - val_accuracy: 0.6713\n",
            "Epoch 4/15\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.7762 - accuracy: 0.7248 - val_loss: 0.8731 - val_accuracy: 0.6994\n",
            "Epoch 5/15\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.7046 - accuracy: 0.7502 - val_loss: 0.9768 - val_accuracy: 0.6713\n",
            "Epoch 6/15\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.6524 - accuracy: 0.7702 - val_loss: 0.7252 - val_accuracy: 0.7605\n",
            "Epoch 7/15\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.5978 - accuracy: 0.7883 - val_loss: 0.9631 - val_accuracy: 0.6958\n",
            "Epoch 8/15\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.5563 - accuracy: 0.8016 - val_loss: 0.7371 - val_accuracy: 0.7573\n",
            "Epoch 9/15\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.5231 - accuracy: 0.8159 - val_loss: 0.7660 - val_accuracy: 0.7474\n",
            "Epoch 10/15\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.4893 - accuracy: 0.8267 - val_loss: 0.7100 - val_accuracy: 0.7655\n",
            "Epoch 11/15\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.4637 - accuracy: 0.8356 - val_loss: 0.7124 - val_accuracy: 0.7643\n",
            "Epoch 12/15\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.4360 - accuracy: 0.8457 - val_loss: 0.7015 - val_accuracy: 0.7773\n",
            "Epoch 13/15\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.4140 - accuracy: 0.8528 - val_loss: 0.8084 - val_accuracy: 0.7509\n",
            "Epoch 14/15\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.3905 - accuracy: 0.8611 - val_loss: 0.6704 - val_accuracy: 0.7847\n",
            "Epoch 15/15\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.3688 - accuracy: 0.8677 - val_loss: 0.6798 - val_accuracy: 0.7913\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ae2910aa500>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained using the fit function, which fits the model to the training data. Here, the model learns to associate the images with labels. It also validates the model on a holdout dataset, the test set, after each epoch.\n",
        "\n"
      ],
      "metadata": {
        "id": "bwzhcxjER4DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "scores = model.evaluate(test_images, test_labels,verbose=0)\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VJiTr_9R49O",
        "outputId": "be28937c-717a-4918-d945-ffc62967f943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7912999987602234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model has been trained, it's important to evaluate its performance on a dataset it hasn't seen before. This is done by using the evaluate function to assess the model's performance on the test dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "N3oE0JfpR6Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('my_model.h5')\n"
      ],
      "metadata": {
        "id": "BQeoAbP3R7Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we save the trained model as an HDF5 file, which is the format preferred by Keras. It will include the model architecture, optimizer, and also the states of all model weights. We can reload the model later with the function keras.models.load_model(filepath).\n",
        "\n",
        "This entire script demonstrates how to build, train, and evaluate a deep learning model for image classification using Keras. The specific model used is a convolutional neural network, which is well-suited for image data.\n",
        "\n",
        "If you would like to test your model, you can find an image of one of the possible outputs, upload it here, and then run the command below. I will not get into the details of the program below because it is not that difficult to replicate and is just there to see how your model did with an image you inputed."
      ],
      "metadata": {
        "id": "FTCG3kPuR9Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.utils as image\n",
        "from PIL import Image,ImageChops\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Load in the original data\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "(train_images_backup, train_labels_backup), (test_images_backup, test_labels_backup) = cifar10.load_data()\n",
        "\n",
        "print(\"CIFAR10 data loaded\")\n",
        "\n",
        "label_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "train_labels_backup = [item for sublist in train_labels_backup for item in sublist]\n",
        "test_labels_backup = [item for sublist in test_labels_backup for item in sublist]\n",
        "\n",
        "# This will work for the models if you download them from the links above.\n",
        "# If you want to export your own models, use the name of them here instead.\n",
        "model_1 = tf.keras.models.load_model('my_model.h5')\n",
        "# model_2 = tf.keras.models.load_model('cnn_model.h5')\n",
        "\n",
        "def plot_image(array, i, labels):\n",
        "  plt.imshow(np.squeeze(array[i]))\n",
        "  plt.title(\" This is \" + label_names[labels[i]])\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()\n",
        "\n",
        "def predict_image(model, x):\n",
        "  x = x.astype('float32')\n",
        "  x = x / 255.0\n",
        "\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_predict = model.predict(x, verbose=0)\n",
        "  print(\"Predicted Label: \", label_names[np.argmax(image_predict)])\n",
        "\n",
        "  plt.imshow(np.squeeze(x))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()\n",
        "\n",
        "  # uncomment this like if you want to see the array of predictions\n",
        "  # print(image_predict)\n",
        "  return image_predict\n",
        "\n",
        "\n",
        "def plot_value_array(predictions_array, true_label, h):\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array[0], color=\"#777777\")\n",
        "  plt.ylim([(-1*h), h])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Code to load an image called 'test3w.jpg'\n",
        "# To use a different image upload it and change the name here.\n",
        "path = \"testfrog.jpg\"\n",
        "img = image.load_img(path, target_size=(32,32), color_mode = \"rgb\")\n",
        "x = image.img_to_array(img)\n",
        "true_label = 6\n",
        "\n",
        "# Predicting the label using model_1.\n",
        "p_arr = predict_image(model_1, x)\n",
        "# This will plot the values on a graph. The last argument is the height of the y-axis.\n",
        "plot_value_array(p_arr, true_label, 1)\n",
        "\n",
        "# Displaying the 100th image from the original dataset\n",
        "plot_image(test_images, 100, test_labels_backup)\n",
        "\n",
        "# Predicting the label using model_1 on an image from test_images\n",
        "img_loc = 100\n",
        "img = test_images[img_loc]\n",
        "x = image.img_to_array(img)\n",
        "p_arr = predict_image(model_1, x)\n",
        "true_label = test_labels_backup[img_loc]\n",
        "print(true_label)\n",
        "plot_value_array(p_arr, true_label, 1)\n",
        "\n",
        "# Create an array of random numbers from 0 to 255 of size (32, 32, 3)\n",
        "x = np.random.randint(0, 255, (32, 32, 3) )\n"
      ],
      "metadata": {
        "id": "sbZbcCzoVBso"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}